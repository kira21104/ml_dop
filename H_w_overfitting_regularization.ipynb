{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Homework assignment (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 1 - Magnitude (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed earlier, regularization methods are expected to constraint the weights (model coefficients).\n",
    "Is it indeed happening?\n",
    "Please do a discovery on your own and find that out empirically (both for L1 and L2). Let's use degree=15 and alpha from ALPHAS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "пример:\n",
    "Исходные данные — 50 точек и у каждой точки 50 признаков. Из 50 признаков только 5 на самом деле имеют значение.\n",
    "\n",
    "Это можно представить формулой: Y = w * X + e, где e — случайная нормальная ошибка, а коэффициенты w = [1, 2, 3, 4, 5, 0, 0, ....,0], где первые пять признаков не нулевые (важные), а остальные нулевые (не имеют влияния на Y). По сути мы этого не знаем, так как у нас есть только X и Y, а веса необходимо вычислить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.34782459  1.33842333  3.38123539  3.98419038  4.54964378 -0.34779579\n",
      "  0.23727981  0.59468316  0.90914552 -0.49891384 -0.54174962  0.72700644\n",
      " -1.50187344 -0.79611303 -1.01014266 -0.3260428  -1.22219963 -0.16707062\n",
      " -1.70864999 -0.88224746  1.77163763 -0.49244268 -1.38597276 -0.96700298\n",
      "  0.0716786   0.29497014 -0.34283277  0.27591254  0.91489396  0.34500143\n",
      " -0.49985173  1.02226848  0.67239694 -0.34740597  0.26708676 -0.11921355\n",
      " -0.5573195   0.83651545 -0.33035223 -0.74446152  0.17590094 -0.42129362\n",
      "  0.48559741  0.05858093  0.84231124 -0.53930678 -0.55865395  0.30747276\n",
      " -0.37639516 -0.42795317]\n",
      "[1. 2. 3. 4. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "\n",
    "N = 50\n",
    "\n",
    "X = np.random.random(size=(N,N)) * 10 + 1\n",
    "\n",
    "w = np.zeros(N)\n",
    "w[:5] = np.arange(5) + 1\n",
    "\n",
    "# Добавляем ошибку\n",
    "e = np.random.normal(0, 1, size=N)\n",
    "\n",
    "# данные об Y\n",
    "Y = np.dot(X, w) + e\n",
    "\n",
    "#Посмотрим на коэффициенты при использовании Линейной Регрессии\n",
    "skm = LinearRegression()\n",
    "skm.fit(X, Y)\n",
    "print(skm.coef_)\n",
    "\n",
    "#Наши начальные коэффициенты выглядят так:\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.7917409   1.81946674  2.85536352  3.88124792  4.9020148  -0.\n",
      " -0.         -0.          0.         -0.          0.         -0.\n",
      " -0.          0.         -0.         -0.          0.00600537 -0.\n",
      " -0.         -0.          0.         -0.         -0.05462118  0.\n",
      " -0.         -0.          0.         -0.         -0.          0.\n",
      " -0.          0.         -0.         -0.         -0.          0.02426293\n",
      " -0.         -0.          0.          0.          0.          0.\n",
      " -0.          0.         -0.         -0.          0.         -0.\n",
      " -0.         -0.        ]\n"
     ]
    }
   ],
   "source": [
    "#Используем L1 регуляризацию и так же посмотрим на полученные коэффициенты\n",
    "\n",
    "las = Lasso()\n",
    "las.fit(X, Y)\n",
    "print(las.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.95723610e-01  1.71653233e+00  2.89932068e+00  3.71394891e+00\n",
      "  4.77405572e+00 -3.13823777e-01 -4.60581844e-02  7.36888196e-03\n",
      "  1.54328783e-01 -2.53686521e-02 -1.26280778e-01  7.93223308e-02\n",
      " -2.05465674e-01  1.83974689e-01  7.18613338e-02 -2.55549061e-01\n",
      " -3.29327757e-02  4.49906451e-02 -2.33911990e-01 -5.47227598e-02\n",
      "  1.50675713e-01 -2.99324922e-01 -1.77284569e-01 -1.67587846e-02\n",
      " -2.19580554e-01 -5.93410894e-02 -3.84269721e-02 -1.77154748e-01\n",
      "  1.13707465e-01  4.79961168e-04  6.41398950e-03  7.98008274e-02\n",
      "  2.09683628e-01  1.03201994e-01  5.26467077e-02  1.24950738e-01\n",
      " -4.15187901e-01  9.45606266e-02  2.68068884e-02  2.61430203e-01\n",
      " -5.38338838e-03  1.95532922e-01 -2.67220170e-02  5.37758175e-02\n",
      "  2.27174874e-02  1.05476065e-01  1.09261514e-02 -1.44452796e-01\n",
      "  7.66974123e-02 -3.91462353e-02]\n"
     ]
    }
   ],
   "source": [
    "#Теперь посмотрим на L2 регуляризацию и коэффициенты, полученные при использовании L2\n",
    "\n",
    "rid = Ridge()\n",
    "rid.fit(X, Y)\n",
    "print(rid.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "Один из способов уменьшить степень переобучения состоит в том, чтобы не допускать очень малых или больших весовых значений для модели. В этом и заключается суть регуляризации.Самый простой метод регуляризации - это добавить штраф к функции потерь пропорционально размеру весов в модели.\n",
    "\n",
    "Как видно из примера, в регуляризации L1 веса уменьшаются на постоянное значение, стремясь к 0. В регуляризации L2 веса уменьшаются на значение, пропорциональное w. Поэтому когда у какого-то веса оказывается большое значение |w|, регуляризация L1 уменьшает вес не так сильно, как L2. И наоборот, когда |w| мало, регуляризация L1 уменьшает вес гораздо больше, чем регуляризация L2. В итоге регуляризация L1 стремится сконцентрировать веса сети в относительно небольшом числе связей высокой важности, а другие веса стремятся к нулю.\n",
    "\n",
    "Процесс сжатия добавляет регрессионным моделям несколько преимуществ:\n",
    "\n",
    "1. Более точные и стабильные оценки истинных параметров.\n",
    "2. Уменьшение ошибок выборки и отсутствия выборки.\n",
    "3. Сглаживание пространственных флуктуаций.\n",
    "Обязательно надо делать шкалирование и центрирование, иначе предикторы с высоким стандартным отклонением будут сильно штравоваться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 2 - Sparsity (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso can also be used for feature selection since L1 is more likely to produce zero coefficients.\n",
    "Is it indeed happening?\n",
    "Please do a discovery on your own and find that out empirically (both for L1 and L2). Let's use degree=15 and alpha from ALPHAS.\n",
    "\n",
    "пример использую из первого задания\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.95655749  1.9853042   2.70814984  3.88683026  4.80775451  0.\n",
      "  0.         -0.         -0.         -0.          0.         -0.\n",
      "  0.         -0.         -0.          0.         -0.         -0.\n",
      "  0.         -0.          0.          0.         -0.         -0.\n",
      " -0.          0.04723858 -0.         -0.          0.         -0.\n",
      "  0.          0.          0.         -0.          0.          0.\n",
      " -0.         -0.          0.         -0.          0.          0.\n",
      " -0.         -0.         -0.          0.          0.          0.\n",
      "  0.         -0.        ]\n"
     ]
    }
   ],
   "source": [
    "#Используем L1 регуляризацию и так же посмотрим на полученные коэффициенты\n",
    "\n",
    "las = Lasso()\n",
    "las.fit(X, Y)\n",
    "print(las.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.22938529e+00  2.25157761e+00  2.49848019e+00  3.94023269e+00\n",
      "  4.71068817e+00  5.99830338e-01  1.78207161e-01  2.48480324e-02\n",
      " -2.13440564e-01 -7.54767860e-02 -2.92102121e-02 -1.07089646e-01\n",
      "  2.58702856e-01 -6.44360377e-02 -6.93859623e-02 -8.48864176e-02\n",
      " -2.33814019e-01 -1.44844777e-01  2.37139692e-01 -4.87403085e-01\n",
      " -6.92091582e-02  1.11694981e-01 -3.82443406e-02  1.97246702e-01\n",
      "  3.27254125e-01  3.92309450e-01 -1.74455493e-01  8.65008340e-02\n",
      " -4.43994344e-02  3.64220618e-02 -3.37057506e-02  6.32256156e-02\n",
      " -2.00828375e-01  3.46924461e-01 -9.21903372e-02  4.32963767e-01\n",
      " -2.25859636e-01 -1.24738652e-02  9.43431196e-02  3.42613519e-02\n",
      "  2.29163976e-01 -6.36890650e-02  3.40260416e-03  5.68110075e-03\n",
      " -9.96914157e-02 -1.34731853e-01 -3.55562573e-03  5.96824424e-03\n",
      " -1.76357247e-01 -1.97536440e-02]\n"
     ]
    }
   ],
   "source": [
    "#Теперь посмотрим на L2 регуляризацию и коэффициенты, полученные при использовании L2\n",
    "\n",
    "rid = Ridge()\n",
    "rid.fit(X, Y)\n",
    "print(rid.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "Как мы видим, L1 регуляризация в большей степени обнуляет ненужные коэффициенты, однако значимые коэффициенты могут быть рассчитаны с чуть большей погрешностью. При этом L2 регуляризация по примеру не обнуляет веса, но старается их уменьшить.\n",
    "\n",
    "1. при L1-регуляризации градиентный спуск будет стремиться к нулю с постоянной скоростью, а достигнув его, там и останется. Вследствие этого L2-регуляризация способствует малой величине весовых коэффициентов, а L1-регуляризация способствует их равенству нулю, тем самым провоцируя разрежённость.\n",
    "2. L1 регуляризация \"стряхивает\" (обнуляет) ненужные коэфициенты. Грубо говоря, она делает нашу модель \"тупее\" для того, чтобы модель искала более простые связи.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excersize 3 - Scaling (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a general rule, it is recommended to scale input features before fitting a regularized model so that the features/inputs take values in similar ranges. One common way of doing so is to standardize the inputs and that is exactly what our pipeline second step (StandardScaler) is responsible for.\n",
    "Why is scaling important? What are the underlying reasons?\n",
    "\n",
    "Как и нормализация, стандартизация очень даже необходима в тех случаях, когда данные имеют входные значения с различными масштабами. Например, некий набор данных содержат столбец \"Возраст\" со значениями по шкале 20-70 и столбец \"Стоимость\" со значениями по шкале 10000-80000. Поскольку эти два столбца отличаются по масштабу, они должны быть стандартизированы, чтобы иметь общий масштаб при построении модели машинного обучения.\n",
    "\n",
    "Так же Скалирование помогает ускорить вычисления в алгоритме,в какой-то мере защищает от выбросов\n",
    "\n",
    "Формально, если объект в наборе данных имеет большой масштаб по сравнению с другими, то в алгоритмах, где измеряется евклидово расстояние, этот объект с большим масштабированием становится доминирующим и его необходимо нормализовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "[[-1. -1.]\n",
      " [-1. -1.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[0, 0], [0, 0], [1, 1], [1, 1]])\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(data)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы увидели, скалирование полезно в том случае, когда хотим сравнить данные, соответствующие различным единицам. Чтобы сделать это согласованным образом для всех данных, необходимо преобразовать данные таким образом, чтобы дисперсия была унитарной, а среднее значение ряда равнялось 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n",
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(scaled_data.mean(axis = 0)) #среднее значение каждого столбца равно 0:\n",
    "print(scaled_data.std(axis = 0)) #стандартное значение каждого столбца равно 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
